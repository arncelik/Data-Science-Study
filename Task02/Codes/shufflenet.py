# -*- coding: utf-8 -*-
"""ShuffleNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hEOskaAmL_I8qNkjocxHT66gSdJQ9r2O
"""

import keras
from keras.preprocessing.image import imageDataGenerator
from keras.applications.imagenet_utilis import preprocess_input
from keras.utilis import plot_model
from shufflenet import ShuffleNet
from keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlatteau, LearningScheduler
import numpy as np

imagedata

def preproces(x):
  x = np.expand_dims(x, axis=0)
  x = prerocess_input(x)
  x /= 255.0
  x -=0.5
  x *=2.0
  return x

if __name__=='__main__':
  groups = 3
  batch_size = 128
  intial_epoch = 0
  ds = '/mt/daten/Development/ILSVRC2012_256'
  
  model = ShuffleNet(groups=groups, pooling='avg')
  plot_model(model, 'model.png', show_shapes=True)
  # model.load_weight('%s.hdf5' % model.name, by_name=True)
  csv_logger = CSVLogger ('%s.log' %model.name, append=(initial_epoch is not 0))
  checkpoint = ModelCheckPoint(filepath='%s.hdf5' % model.name, verbose=1,
                              save_best_only=True, monitor="val_acc", mode="max")
  
  learn_rates = [0.05, 0.01, 0.005, 0.001, 0.0005]
  lr_scheduler = LearningRateScheduler(lambda epoch: learn_rates[epoch // 30])
  
  train_datagen = ImageDataGEnerator(preprocessing_fucntion = preprocess,
                                    zoom_range = 0.05,
                                    width_shift_range = 0.05,
                                    heigth_shift_range = 0.05,
                                    horizontal_flip=True)
  test_datagen = ImageDataGenerator(preprocessing_function = prerocess)
  
  train_generator = train_datagen.flow_from_directory(
     '%s/train/' %ds,
      target_size=(224, 224),
      batch_size = batch_size)
  
  test_generator = test_datagen.flow_from_directory(
            '%s/val/' % ds,
            target_size=(224, 224),
            batch_size=batch_size)

  model.compile(
              optimizer=keras.optimizers.SGD(lr=.05, decay=5e-4, momentum=0.9),
              metrics=['accuracy'],
              loss='categorical_crossentropy')

  model.fit_generator(
            train_generator,
            steps_per_epoch=train_generator.samples // batch_size,
            epochs=100,
            workers=7,
            initial_epoch=inital_epoch,
            use_multiprocessing=False,
            validation_data=test_generator,
            validation_steps=test_generator.samples // batch_size,
            callbacks=[csv_logger, checkpoint, lr_scheduler])

